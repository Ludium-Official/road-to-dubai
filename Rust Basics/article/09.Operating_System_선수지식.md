# Operating System Basics for Asynchronous/Parallel Programming

## Learning Objectives

- Understand the basic concepts of operatino system and its relationship with async/parallel programing of Rust
- Learn the differences between Processes and Threads and their usage in Rust
- Learn about context switching and scheduling and their relationship with Rust's asynchronous execution
- Learn about synchronization techniques and their relationship with Rust's concurrency primitives 
- Learn about memory management and virtual memory and their relationship with Rust's memory management Safety
- Compare I/O models and Rust's asynchronous I/O implementation methods
- Learn how to apply operating system concepts in actual Rust code

## Operating Systems and Rust's Asynchronous/Parallel Programming


<img width="802" alt="image" src="https://github.com/user-attachments/assets/3214f8f5-a746-4b38-88fb-9ec000985a37">

Operating system is a system software that primarily manages computer hardware and provides various functions to applications. Understanding operating systems is necessary to utilize Rust's asynchronous and parallel capabilities in full effectivness and also to understand Rust's advantages. It manages hardware resources as shown in the figure above.

## Two CPU Modes

<img width="775" alt="image" src="https://github.com/user-attachments/assets/f1d10765-05a2-497d-b967-2e0483a9165b">

CPU has two modes: User Mode & Kernel Mode. Mode is set up by status bit in protected register

### User Mode
User mode is basically where normal applications are run. 
In user mode accessing hardware resources is prohibited due to limited access and cannot modify important parts of the system

Examples: Web browsers, word processors, and other general application execution, user-level library function calls

### Kernel Mode
Kernel Mode is where operation system code(kernel) is runs (a.k.a called Supervisor Mode or System Mode).

In Kernel mode every all hardware and CPU instructions are accessible (through kernel program execution!) 

Examples: Memory management, process scheduling, file system management

If a User Program is in need of certain jobs which requires resources that are accessible through kernel ex) I/O, then mode switching is required. 

This mode change occurs in the following situations: 

(1) Hardware Interrupt - For example, Timeout interrupt

(2) Software interrupt (exception) - Error occurrence

(3) System call - When requested by user

To summarize, assuming I'm a process, (1) and (2) are handling of processes that are unrelated to me, and (3) is a kernel privilege request that I (the user) am requesting. However, 2 and 3 cannot be strictly separated.

Example
<img width="693" alt="image" src="https://github.com/user-attachments/assets/12deab84-0bf0-4992-8ed0-cad649e87f8a">

## OS Roles 

(1) User Service
provides various user-friendly services.

- Loading Program in a memory + Program Execution  
- I/O Operations
- File system File reading, writing
- Communications
  Communication between physically different systems via network, or between processes within one computer

- Error Detection & Handling
  
(2) Resource Allocation
  Allocates resources for Multiple Users (e.g., server systems) or Multiple tasks (multiple processes)

(3) Accounting
  Can measure usage for users and resources
(4) Protection
  Ensures safety of system control/access

## System Call 
Running Program(User Program)and OS interacts through system call api(application programming interface)

<img width="690" alt="image" src="https://github.com/user-attachments/assets/09d82c62-9823-4369-865c-489e0ceda79f">

This interface (abstraction) makes it easier for user program developers to develop without knowing OS functions in detail (in fact, since syscall requires knowledge of kernel functions, programming language libraries wrap syscall to provide user libraries), and improves system security and portability.

POSIX API is an example of this. POSIX API abstracts syscalls. 
![image](https://github.com/user-attachments/assets/405b3ce9-2430-4b63-9a70-1ef53214d410)


## Process

A process can be understood as the unit by which the OS manages programs. The definition of a process is as follows:

<img width="406" alt="image" src="https://github.com/user-attachments/assets/3b02f5df-d7aa-4626-8842-d1104e0691ae">  

A process is consisted of following elements:

1. images  
   -Code: Machine Langauge  
   -Data: Variable  
   -Stack: States for function calls  
   -Heap: dynamic memory

   ![image](https://github.com/user-attachments/assets/1875bc5b-b822-420a-977d-14c71bfa74c1)

  
2. Process context  
   -Program Context: data registers, pc, sp ...    
   -Kernel Context: pid, gid, open files, paging tables ... 

With all theses data PCB(Process Control Block) is Created 

PCB elements are designed using the previously mentioned Images and Process Context. In other words, it's an abstract data structure designed to manage processes.

![image](https://github.com/user-attachments/assets/30cde532-ec60-4fdb-a69a-40f92a1ffc77)

Simply put, All that a software does is designing data structures and effectively modifying those data structures.   
Operating systems are no exception—they are just software. They design a data structure called a PCB (Process Control Block) and use it to manage abstracted processes. Ultimately, following the Stored Program Concept, the kernel is also loaded into main memory and operates in the same way.
 
The kernel also manages other data structures:

- Memory data structures: Process memory allocation, disk allocation for processes, virtual memory info
- I/O data structures: I/O device availability, I/O operation status, main memory addresses for I/O transfer sources or destinations
- File data structures: Current states of files, file disk addresses


Below is the Lifetime of a process 
![image](https://github.com/user-attachments/assets/a88308df-af19-4b10-9221-dbe936b1c383)

Os also schedules 
- Multi-Programming Degree
- CPU Allocation
- swap Out/in


### Process switching vs Mode switching
process swithching happens significantly less than mode switching, process switching faces overheads such as save/loading process context 

<img width="421" alt="image" src="https://github.com/user-attachments/assets/b8ce8d77-be06-4ffd-a4df-dd6dc6ed472d">


### Process Managing
Processes have various lifecycle events like creation, exit, resource sharing, etc. 

In my personal opinion, the best data structure to manage(almost any for of data) is the tree data structure. The reason being is that its dependency is straight-forward, allowing for comprehensive yet detailed management under the jurisdiction of each instance. That’s why, when you look at the graphs of centralized systems in history books, they are often tree structures. (Haha!)

![image](https://github.com/user-attachments/assets/64b2cdfc-ed58-4dc3-8f2d-6a1b4f0854ff)

Enough with the pep talk, Processes are create and managed like the picutre below. There is a relationship between a parent process and its child process.  

![image](https://github.com/user-attachments/assets/320bf2e9-7e72-4de5-8f33-bf3c5a87c72b)


## Excution of OS

As mentioned earlier, according to the Stored Program Concept, OS is also a program and will be loaded as a process. In other words,  `OS is just subject for Scheduling`.

Then who has control over the OS?
-> It depends on the OS design.

Non-Process Kernel is not executed as a process 
<img width="533" alt="image" src="https://github.com/user-attachments/assets/48381f9d-1a8a-44f6-b2b4-c17a108ff615">

User Process containing Kernel: Switching is delegated to PSF outside the process
<img width="550" alt="image" src="https://github.com/user-attachments/assets/43c3e9e7-fdff-4d2d-8e3e-d4b8746005bb">

Separate OS Process: Also delegates to PSF outside the process. This model is suitable for multiprocessor environments.
<img width="587" alt="image" src="https://github.com/user-attachments/assets/ca31dca8-0a28-4bb1-9def-97b0c884c1d8">

## Multithreading
- Process model's tree hierarchy structure was too heavy for handling multiple client requests in a web server

- Traditional processes could only use one CPU at a time, limiting multiprocessor architecture benefits. Only one CPU was used to run a process 

Therefore, the Multiple Threads Model was proposed:

- Unlike processes that has its own images and context, In Multiple Threads Model threads only consist of Stack + Thread Context (sp, pc, local variables, return register)
- Threads share code and most data among themselves, including context

Modern Process Model :
<img width="802" alt="image" src="https://github.com/user-attachments/assets/d863aef8-1efb-4259-9cc0-1c8a59b641bd">

From a logical structure perspective, processes are managed in a tree-like form, while threads are managed in a parallel relationship.

<img width="661" alt="image" src="https://github.com/user-attachments/assets/dd7123d3-e9f3-4de4-93a7-3919edb63bed">

Similarities between Threads and Processes:
- Each has its own logical flow
- Each is scheduled independently

Differences:

- Threads share code and data among themselves
- Threads have no hierarchy and are lightweight, making them easier to create
- Unlike IPC, threads can communicate without kernel intervention as they share address space and memory

Through this, threads that make up a process can each be allocated a CPU, enabling parallel processing for a single process. 

### User-Level Thread vs Kernel-Level Thread

#### User-Level Thread
Thread management is done by user-level threads library, and the kernel is unaware of these threads.
Advantages:

- Lightweight (all lifecycle events occur in user space without kernel intervention)
- Context switching has less overhead as it doesn't require kernel mode transition

Disadvantages:

- I/O in any thread blocks the entire process
- Cannot take advantage of multiprocessors

![image](https://github.com/user-attachments/assets/a3c7c0c3-111e-47dc-8c69-2f2188eca39b)

#### Kernel-Level Thread
OS-managed threads
Advantages:

- Can utilize multiprocessor benefits as kernel is aware of them
- One thread's I/O doesn't block others

Disadvantages:

- Library thread creation triggers kernel thread creation  

![image](https://github.com/user-attachments/assets/5b0336f2-5182-4a4e-95af-04039ae022ee)

A well-mixed approach is the M:N
thread model, with Go's goroutines being a representative example.   

https://medium.com/@rezauditore/introducing-m-n-hybrid-threading-in-go-unveiling-the-power-of-goroutines-8f2bd31abc84

Rust uses a 1:1 Thread model due to its Ownership system, which allows easy implementation of high-performance parallelism through convenient concurrent problem solving.  

https://doc.rust-kr.org/ch16-01-threads.html



## Multiprocessor scheduling

Multiprocessor scheduling involves efficiently utilizing multiple CPUs. Key considerations include:

1. Cache affinity
   - Running processes on previously used CPUs improves cache hit rates
   - Significant state information accumulates in CPU cache and TLB

2. Load balancing
   - Work must be distributed evenly across CPUs
   - Uneven distribution leads to performance degradation

3. Parallelism
   - Parallel-capable tasks should be assigned to different CPUs
   - Must schedule multiple threads simultaneously for multithreaded applications

4. Synchronization
   - Shared data access across CPUs requires synchronization
   - Must use mutual exclusion techniques like locks

5. Scalability
   - Scheduler performance shouldn't degrade significantly with increased CPUs
   - Must minimize lock contention overhead


These are the multiprocessor scheduling algorithm

1. SQMS: Single-Queue Multiprocessor Scheduling
- Uses a single central queue to manage all tasks
- Advantages:
  - Simple implementation. Easy to extend existing single CPU schedulers
  - Natural load balancing occurs
- Disadvantages:
  - Scalability issues: As CPU count increases, lock contention on the central queue leads to performance degradation
  - Cache affinity problems: Process efficiency decreases as they move between different CPUs

Example of SQMS:
```
Queue: A B C D E NULL

CPU 3: D C B A E ... (repeat) ...
CPU 2: C B A E D ... (repeat) ...
CPU 1: B A E D C ... (repeat) ...
CPU 0: A E D C B ... (repeat) ...
```

In this case, cache affinity decreases as each task continuously moves between CPUs.

2. MQMS: Multi-Queue Multiprocessor Scheduling
- Uses separate queues for each CPU
- Advantages:
  - Good scalability. Less queue contention even with increased CPU count
  - Good cache affinity. Tasks remain on the same CPU, allowing effective cache usage
- Disadvantages:
  - Load balancing issues may occur. Workload differences can develop between queues
  - More complex implementation

Example of MQMS:
```
Q0: A C    Q1: B D

CPU 0: A A C C A A C C ...
CPU 1: B B D D B B D D ...
```

In this case, cache affinity is good as tasks stay on the same CPU, but load imbalance may occur when task C completes.

3. Work Stealing
- A technique to solve the load balancing problem in MQMS
- When one CPU's queue is empty, it takes tasks from other CPU queues
- Periodically checks other queues' status and moves tasks when necessary
- Advantages:
  - Maintains MQMS benefits while improving load balancing
- Disadvantages:
  - Frequency of queue checking is crucial. Too frequent checks create overhead, too infrequent checks prolong imbalances

## Linux Schedulers

Linux uses various schedulers:

### 1. O(1) Scheduler
- Uses priority-based multiple queues
- Each CPU has two priority queues (active and expired)
- Can make scheduling decisions in constant time (O(1))

### 2. Completely Fair Scheduler (CFS)
- Uses proportional fair scheduling with multiple queues
- Uses red-black trees to quickly select processes with least runtime
- Assigns weights to each process for fair CPU time distribution

### 3. BF Scheduler (BFS)
- Uses a single queue approach
- Based on EEVDF (Earliest Eligible Virtual Deadline First) algorithm
- Lower scalability but good responsiveness and simple implementation


## Synchronization
We have understood Multithreading and CPU scheduling. But are there any problems when Data is Shared between Threads?  

<img width="697" alt="image" src="https://github.com/user-attachments/assets/67add7ec-dd1c-479d-99b8-07abe0fb2e62">

This is called a concurrency issue. If two or more Threads share resources without synchronization Problems arise. 
Hence, we need a synchronization mechanism

Which resources are shared? 
- Between processes: 
  Shard memory objects, files . .. 
- Between threads: 
  Global variables on static data segment
  dynamic objects on heap

### Critical Sections
Critical sections are regions where shared data is accessed. They must ensure:

- Mutual Exclusion
- Bounded Waiting
- Progress

#### Structure
repeat
`entry section`
  critical section
`exit section`
  remainder section
until false;


해당 솔루션에는 `Lock`, `Semaphore`, `Monitor`등이 있다. 차례로 알아보자.

### Locks(low-level synchronization primitive)
두가지 operation을 제공하는 동기화 메커니즘이다.
`acquire()/lock()` and `release()/unlock()`.
Critical Section에 진입 전에 lock()을 요청하고, 나와서는 unlock()을 한다. 

이 방식은 Peterson's algorithm, Atomic 연산 보장, critical section 중에 context switching을 요청하는 시스템 interrupt 방지 등으로 인해서 구현된다.  
그리고 보통 `spinlock`을 통해서 진입 전에 process는 무한 루프를 돌며 대기해야한다. 

### Semaphore
spin lock처럼 busy waiting을 할 필요없이 process 연관 큐를 가진 채로 거기다가 재우고 깨운다.

```rust
 wait(S) {
     S--;
     if S < 0
         // 이 프로세스를 재움 큐에 추가 (잠 듦)
 }

 signal(S) {
     S++;
     if S <= 0
         // 재움 큐로부터 프로세스를 제거 (깨어남)
 }
```

하지만, 이 Wait과 Signal 연산에 대해서 또 동기화 문제가 발생한다. 따라서 또 다시 critical section이 발생하는데, 아주 작으므로 해당 critical section에 대해서는 좀 더 정교한, 하드웨어 솔루션으로 접근이 가능하다. 
Binary Semaphore를 mutex라고 부른다. 

Semaphore는 좋은 방법이지만, 데드락을 발생시킬 수 있다는 문제점이 있다.

#### 철학자 문제 (Dining Philosophers Problem)
철학자 문제는 동시성 문제를 설명하는 유명한 예시다. 
문제의 설정은 다음과 같다:
5명의 철학자가 원형 테이블에 앉아 있다.
각 철학자 사이에 포크가 하나씩 있다 (총 5개의 포크).
철학자는 생각하거나 먹는 두 가지 행동만 한다.
먹기 위해서는 양쪽의 포크 두 개를 모두 사용해야한다.

![출처: 나무위키](https://github.com/user-attachments/assets/b0dda31f-3f17-4743-8580-86e15536bbc0)


이 문제에서 데드락이 발생할 수 있는 상황은 다음과 같다:

모든 철학자가 동시에 왼쪽 포크를 집는다.
각 철학자는 오른쪽 포크를 기다리지만, 모든 포크가 사용 중이므로 영원히 기다리게 된다.
물론, 데드락의 4가지 조건 중 하나씩 깨면서 데드락을 해결하는 해결책이 존재한다! 

데드락의 4가지 조건
- Mutual Exclusion
  한번의 하나의 process만 resource를 이용할 수 있다
- Hold and Wait
  자원을 하나라도 가지고 있는 process가 다른 자원을 받기 위해 기다리는 상황 
- No Preemption
  자원을 자발적으로 내려놓을 때까지 기다려야한다.(선점이 불가) 
- Circular Wait
  resource 할당 그래프에 cycle이 있다 

해결책은 알아서 찾아보도록 하자.

## Ownership System과 동기화 문제 
Rust's compiler can't catch all runtime issues, 
However rust prevents data race conditions at compile time 
This is one of Rust's strongest features, making multithreaded programming much safer. 

However, other concurrency issues like deadlocks and livelocks can still occur at runtime, so it is best to be aware of those conditions. 












